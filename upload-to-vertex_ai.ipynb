{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397ccab6-55b0-4bc1-84a5-312e7e9e5af9",
   "metadata": {},
   "source": [
    "# Deploy your YOLO Model with NVIDIA's Triton Server to a Vertex AI Endpoint\n",
    "\n",
    "**Note**: \n",
    "\n",
    "This guide assumes that:-\n",
    "- The appropriate model artifacts (in this case the `models` directory) are already stored on GCS. \n",
    "-  You have built the Docker image using the Dockerfile in the repo. \n",
    "- The Triton Server (with the YOLO model) you are about to deploy to Vertex AI Endpoints has been tested locally.\n",
    "\n",
    "If you haven't done the following, I would recommend reading the `README.md` in the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a355c-6c79-45ac-8a6f-66f27c4b9053",
   "metadata": {},
   "source": [
    "## 1. Initializations\n",
    "\n",
    "In this section, we would initialize the aiplatform client and declare other variables like the <a href= \"https://cloud.google.com/kms/docs/key-management-service\"> KMS key </a> (if your org uses one), etc.\n",
    "\n",
    "Before proceeding please make sure you install google-cloud-aiplatform. \n",
    "If you want to exactly follow my setup, please run `pip install google-cloud-aiplatform==1.79.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ebf3c8-8c07-4f06-879b-a53267f2c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "KMS_KEY = \"\" # Enter your KMS_KEY if you have one\n",
    "location = \"\" # Enter the location/region for example 'us-east4'\n",
    "aiplatform.init()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06909fe6-f34f-404e-bf67-98ccc3f689a9",
   "metadata": {},
   "source": [
    "## 2. Upload the Docker image to Google Cloud Artifact Registry (GAR)\n",
    "\n",
    "You must use GAR to upload your docker image because Vertex AI's Model Registry only integrates with GAR. And, Vertex AI Endpoint pulls the \"model\" from Vertex AI's Model Registry. \n",
    "\n",
    "Please make sure you have enabled Google Cloud Artifact Registry in your project and can upload docker images to it. If not, please refer to this <a href=\"https://cloud.google.com/artifact-registry/docs/docker/pushing-and-pulling\"> link </a>\n",
    "\n",
    "**Steps:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52234ac8-b746-4246-be00-32c023c866c0",
   "metadata": {},
   "source": [
    "1. Tag the Docker image to include the artifact registry location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad2681-d9d5-4929-8b5e-92c66e7d773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the command below and run it. \n",
    "# location can be us-east4, us-central1, etc.\n",
    "! docker tag <docker image> <location>-docker.pkg.dev/<project name>/<registry name>/<docker image> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4179aee-dfcd-40bf-a437-5352580b298d",
   "metadata": {},
   "source": [
    "2. Push the newly tagged Docker image to GAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e100adf7-f3ed-484f-aea7-e001f157b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the command below and run it. \n",
    "! docker push <location>-docker.pkg.dev/<project name>/<registry name>/<docker image> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a51e82-2ead-44c0-845e-df78d1a2e0da",
   "metadata": {},
   "source": [
    "## 3. Register the YOLO model on Vertex AI's Model Registry\n",
    "\n",
    "Since Vertex AI has a default integration with NVIDIA Triton models, we can simply state the GCS location of the model artifacts and the URI of the Docker image on GAR along with other nominal details. The `model.resource_name` will be used while \"mounting\" our model on a Vertex AI Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6107325e-7b35-44dd-bb66-97f3194673fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit and Run\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name                 = \"\", # Example:'yolo-model'\n",
    "    description = \"\", # Example: 'YOLO model for document layout identification'\n",
    "    artifact_uri                 = \"<gcs url of model artifacts (in this case NVIDIA Triton model registry>\", \n",
    "    serving_container_image_uri  = \"<docker image uri from step 2.>\",\n",
    "    # Uncomment the line below if you have a KMS Key\"\n",
    "    # encryption_spec_key_name     = KMS_KEY,\n",
    "    location = location\n",
    ")\n",
    "print(\"Model resource:\", model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a941553-e284-4b2c-993f-75a72dc9bfde",
   "metadata": {},
   "source": [
    "## 4. Create a Vertex AI Endpoint\n",
    "\n",
    "The following code creates the default Vertex AI Endpoint type called <a href = \"https://cloud.google.com/vertex-ai/docs/predictions/create-public-endpoint#create_a_shared_public_endpoint\"> 'a shared public endpoint'.</a>\n",
    "The `endpoint.resource_name` will be used for \"mounting\" the model from the Model Registry onto a Vertex AI Endpoint in step 5.\n",
    "\n",
    "*Note*: Please refer to the following <a href=\"https://cloud.google.com/vertex-ai/docs/predictions/choose-endpoint-type\"> page </a> to choose the endpoint that best suits your needs/requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65e9a63-9cc2-4da6-b432-ac69695d642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name= \"\",  # Example: 'yolo-model-endpoint' \n",
    "    # Uncomment the line below if you have a KMS Key\n",
    "    # encryption_spec_key_name = KMS_KEY,\n",
    "    location = location\n",
    ")\n",
    "print(\"Endpoint resource:\", endpoint.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8bec4-807d-48a4-8e46-ad5dc6f4e2a2",
   "metadata": {},
   "source": [
    "## 5. Upload the YOLO model from Vertex AI Model Registry onto our created Vertex AI Endpoint\n",
    "\n",
    "The following code/command uploads our model from the Model Registry from step 3 to our endpoint created in step 4. \n",
    "The JSON payload will contain information about the model to upload, replicas, machine types, GPUs, and auto-scaling logic. A post request with the created JSON payload will be sent to Vertex AI endpoint created in step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f9585f-74da-4934-ae7a-0bada352a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to edit the JSON values apart from the model_name. \n",
    "model_name = model.resource_name\n",
    "model_serving = {\n",
    "    \"deployedModel\": {\n",
    "      \"model\": f\"{model_name}\",\n",
    "      \"displayName\": \"yolo-model\",\n",
    "      \"dedicatedResources\": {\n",
    "         \"machineSpec\": {\n",
    "           \"machineType\": \"g2-standard-8\",\n",
    "           \"acceleratorType\": \"NVIDIA_L4\",\n",
    "           \"acceleratorCount\": \"1\"\n",
    "         },\n",
    "         \"minReplicaCount\": 2,\n",
    "         \"maxReplicaCount\": 6, \n",
    "         \"autoscalingMetricSpecs\": [\n",
    "        {\n",
    "          \"metricName\": \"aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle\",\n",
    "          \"target\": 25\n",
    "        }\n",
    "      ]\n",
    "       }\n",
    "    }\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c65934a-143f-4154-8033-888fde1f6af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"model_serving.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    json.dump(model_serving, fp)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ad366-f9b3-4444-ab09-cd746c617131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the endpoint.resource_name in the curl command according to the endpoint.resource_name in step 4. \n",
    "!curl -X POST \\\n",
    "     -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "     -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "     -d @model_serving.json \\\n",
    "     \"https://<location>-aiplatform.googleapis.com/v1/<endpoint.resource_name>:deployModel\""
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
